---
title: "Hotspot"
author: "Taiyuan Zhang"
date: "2023-07-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Parameter change

```{r}
#K-num iteration Sets the value of K (row) that will be tested during this run.
Kmin <- 20  
Kmax <- 40 
iteration <- 20 #iteration number per k num 
topnum = 20 
file_names <- c("output_1.csv", "output_2.csv", "output_3.csv", "output_4.csv", "output_5.csv", "output_6.csv", "output_7.csv", "output_8.csv", "output_9.csv", "output_10.csv", "output_11.csv", "output_12.csv", "output_13.csv", "output_14.csv", "output_15.csv", "output_16.csv", "output_17.csv", "output_18.csv", "output_19.csv", "output_20.csv") # File name of the output result, the number must be the same as topnum (but the final output result number may be less than the topnum number, because some non-significant K values will be discarded in model validation)
alpha = 0.05 # significance
alp = 0.95  
```
Parameter change: at row 414(K-num iteration) topnum, filename，row 677,993 (alpha for significance test), row 699,1020 n_permutations for significance of permutation. 808,1120 alp(alpha for high-mutation point test, and per_num, which is very important),

## Load library

```{r cars, echo=FALSE, include=FALSE}
library(sp)
library(spgwr)
#library(spdep)
library(readxl)
#library(spatialreg)
library(gstat)
library(stats)
library(dbscan)
library(cluster)
library(ggplot2)
library(scatterplot3d)
library(rgl)
library(mclust)
library(plotly)
library(dplyr)
library(FNN)
library(matrixStats)
library(tidyr)
library(scales)
library(factoextra)
library(spatstat)
library(clValid)
library(factoextra)
library(fpc)
library(NbClust)
library(readxl)
library(plot3D)
library(DT)
# for Kolmogorov-Smirnov test
library(ks)
library(gridExtra)
#library(writexl)
```

## Load data

Load data with spatial coordinates and mutation rates

```{r, echo=FALSE}

#setwd("/Users/ryan/Documents/GitHub/Shenzhen Bay lab/spike_protein_3structures_markerd_with_variants_from_cncb")
data <- read_excel("6vxx_variants.xls")


data<-na.omit(data) #omit na value
data_matrix<-as.matrix(data)
data2<-data[1:972,c("X","Y","Z","virusPercent")]

```


```{r, echo=FALSE}

data <- read_excel("6vxx_variants.xls")

sub_data<-data[1:972,c("X","Y","Z","virusPercent")]
sub_data_1<-data[1:972,c("X","Y","Z")]
summary(sub_data)

```

##Prove necessary to do normalization

#Method 1 : two sample t test

Caculate each dimension's mean and standard error

mean

```{r, echo=FALSE}
apply(sub_data,2,mean)

```

standard error

```{r}
apply(sub_data,2,sd)
```

H0 : equal mean H1 : not equal mean

if p-value small, then 2 covariates has significant difference between mean, which means it requires normalization.(scale)

```{r, echo=FALSE}

t.test(sub_data[, 1], sub_data[, 2])
t.test(sub_data[, 1], sub_data[, 3])
t.test(sub_data[, 2], sub_data[, 3])

```

#Method 2: correlation analysis

```{r, echo=FALSE}

# Variance analysis space difference
data_df<-as.data.frame(sub_data_1)

# Calculate the Pearson correlation coefficient
cor_mat <- cor(data_df, method = "pearson")

# View the correlation coefficient matrix
print(cor_mat)

# The correlation coefficient ranges from -1 to 1,
# The greater the absolute value, the stronger the correlation, while the plus or minus sign indicates the direction of the correlation

for (i in 1:(ncol(data_df) - 1)) {
  for (j in (i + 1):ncol(data_df)) {
    cor_test <- cor.test(data_df[, i], data_df[, j], method = "pearson")
    cat("correlation between", names(data_df)[i], "and", names(data_df)[j], "is", cor_test$estimate, "with p-value", cor_test$p.value, "\n")
  }
}

# Calculate Kendall grade correlation coefficient
cor_mat_2 <- cor(data_df, method = "kendall")

#correlation matrix
print(cor_mat_2)

# The Kendall grade correlation coefficient and its significance level between multiple variables are calculated

for (i in 1:(ncol(data_df) - 1)) {
  for (j in (i + 1):ncol(data_df)) {
    cor_test <- cor.test(data_df[, i], data_df[, j], method = "kendall")
    cat("correlation between", names(data_df)[i], "and", names(data_df)[j], "is", cor_test$estimate, "with p-value", cor_test$p.value, "\n")
  }
}

# If the significance level is very small (usually less than 0.05),
# Then we can consider the correlation to be significant, that is, at the population level, there is a significant correlation between these two variables.

```

Results show we need to do normalization

##Normalization validation

#Check data range and distribution
It's able to normalize if the data follows same distribution before and after the normalization.

Firstly, normalise x,y,z

```{r,echo=FALSE}
xyz<-data2[,c("X","Y","Z")]
#normalize
xyz_norm<-scale(xyz)

# The normalized coordinates and abrupt frequency are combined into characteristic vectors
feature_vector <- cbind(xyz_norm, data2$virusPercent)

data2 <- data.frame(data2)

```

Here we prove that x,y,z follows this rule.

```{r}
plot3d(feature_vector[,1], feature_vector[,2], feature_vector[,3], col = "blue", size = 2)
plot3d(data2[,1], data2[,2], data2[,3], col = "blue", size = 2)
```

Check original xyz and normalised xyz's data range and distribution

```{r,echo=FALSE}

summary(data2[, 1:3])  # Shows the range and distribution
hist(data2[, 1])      # Plot the histogram of column x
hist(data2[, 2])       
hist(data2[, 3])       

```

check the data range and distribution after normalization

```{r,echo=FALSE}
# View the data range and distribution of the standardized data
summary(feature_vector[, 1:3])  #  
hist(feature_vector[, 1])       
hist(feature_vector[, 2])    
hist(feature_vector[, 3])     
```

```{r}

# compare distribution of x after normalizing
ks.test(data2[, 1], feature_vector[, 1])  #  Kolmogorov-Smirnov test

# compare distribution of y after normalizing
ks.test(data2[, 2], feature_vector[, 2])   

# compare distribution of z after normalizing
ks.test(data2[, 3], feature_vector[, 3])   

```

Here we prove that x,y,z,vp follows the rule

```{r,echo=FALSE}
# density plot before standardization
p1 <- ggplot(data2, aes(x = X)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("X") + ylab("Density") + ggtitle("Original X Density Distribution")

p2 <- ggplot(data2, aes(x = Y)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("Y") + ylab("Density") + ggtitle("Original Y Density Distribution")

p3 <- ggplot(data2, aes(x = Z)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("Z") + ylab("Density") + ggtitle("Original Z Density Distribution")
feature_vector<-data.frame(feature_vector)

# density plot after standardization
p4 <- ggplot(feature_vector, aes(x = X)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("X") + ylab("Density") + ggtitle("Standardized X Density Distribution")

p5 <- ggplot(feature_vector, aes(x = Y)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("Y") + ylab("Density") + ggtitle("Standardized Y Density Distribution")

p6 <- ggplot(feature_vector, aes(x = Z)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("Z") + ylab("Density") + ggtitle("Standardized Z Density Distribution")

# arrange
grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 2, ncol = 3)

```

```{r,echo=FALSE}

# Loop all two-dimensional multivariable density plots
# 2D density map before standardization

library(ggplot2)
p1 <- ggplot(data2, aes(X, Y)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "X", y = "Y") +
  theme_bw()

p2 <- ggplot(data2, aes(X, Z)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "X", y = "Y") +
  theme_bw()

p3 <- ggplot(data2, aes(Y, Z)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "Y", y = "Z") +
  theme_bw()


p4 <- ggplot(feature_vector, aes(X, Y)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "X", y = "Y") +
  theme_bw()

p5 <- ggplot(feature_vector, aes(X, Z)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "X", y = "Z") +
  theme_bw()

p6 <- ggplot(feature_vector, aes(Y, Z)) + 
  geom_density_2d(alpha = 0.6) +
  labs(x = "Y", y = "Z") +
  theme_bw()

 
grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 2, ncol = 3)

```

#Transformation method 1: z-norm

```{r,echo=FALSE}

# Conversion Method 1: Simultaneous standardization
# Extraction mutation frequency
w<-data2[,c("virusPercent")]

w_norm<-scale(w)
# Combine the normalized coordinates and mutation frequency into feature vectors
scale_vector <- cbind(xyz_norm, w_norm)
# Observe the data distribution before and after the normalization 
p1 <- ggplot(data2, aes(x = virusPercent)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("virusPercent") + ylab("Density") + ggtitle("Original vP Density Distribution")
p1<-p1+xlim(0,0.02)

scale_vector<-data.frame(scale_vector)

p2 <- ggplot(scale_vector, aes(x = V4)) +
  geom_density(fill = "steelblue", color = "white") +
  xlab("virusPercent") + ylab("Density") + ggtitle("standardized vP Density Distribution")
p2<-p2+xlim(-0.2,-0.1)

grid.arrange(p1, p2, nrow = 1, ncol = 2)

#histogram
hist(data2[,4])
hist(scale_vector[,4])

ks.test(data2[, 4], scale_vector[, 4])  # 进行Kolmogorov-Smirnov检验，比较z列的分布

```

#Transformation Method 2: min-max

```{r,echo=FALSE}
# Transformation Method 2: Feature reduction
# Min-Max scale x, y, z coordinate values
data2$x <- (data2$X - min(data2$X)) / (max(data2$X) - min(data2$X))
data2$y <- (data2$Y - min(data2$Y)) / (max(data2$Y) - min(data2$Y))
data2$z <- (data2$Z - min(data2$Z)) / (max(data2$Z) - min(data2$Z))
data2$mutation <- (data2$virusPercent - min(data2$virusPercent)) / (max(data2$virusPercent) - min(data2$virusPercent))

new_data <- data.frame(x = data2$x, y = data2$y, z = data2$z, mutation = data2$mutation)

# Verify the data distribution similarity before and after feature scaling

plot3d(new_data[,1], new_data[,2], new_data[,3], col = "blue", size = 2)

# scatter plot
pairs(data2[, c("X", "Y", "Z", "virusPercent")], main = "Scatterplot Matrix before")
pairs(new_data[,c("x","y","z","mutation")],main = "Scatterplot Matrix after")

# compare histogram
par(mfrow = c(2, 4))   
hist(data2$virusPercent, main = "Mut Before Scale", xlab = "Mutation")
hist(data2$X, main = "X Before Scale", xlab = "X")
hist(data2$Y, main = "Y Before Scale", xlab = "Y")
hist(data2$Z, main = "Z Before Scale", xlab = "Z")
hist(new_data$mutation, main = "Mut After Scale", xlab = "Mutation")
hist(new_data$x, main = "X After Scale", xlab = "X")
hist(new_data$y, main = "Y After Scale", xlab = "Y")
hist(new_data$z, main = "Z After Scale", xlab = "Z")

# compare density plot
par(mfrow = c(2, 4))  
plot(density(data2$virusPercent), main = "Mut Before Scale", xlab = "Mutation")
plot(density(data2$X), main = "X Before Scaling", xlab = "X")
plot(density(data2$Y), main = "Y Before Scaling", xlab = "Y")
plot(density(data2$Z), main = "Z Before Scaling", xlab = "Z")
plot(density(new_data$mutation), main = "Mut After Scale", xlab = "Mutation")
plot(density(new_data$x), main = "X After Scale", xlab = "X")
plot(density(new_data$y), main = "Y After Scale", xlab = "Y")
plot(density(new_data$z), main = "Z After Scale", xlab = "Z")

# compare qq-plot
par(mfrow = c(2, 4))    
qqnorm(data2$virusPercent, main = "Mut Before Scale")
qqline(data2$virusPercent)
qqnorm(data2$X, main = "X Before Scale")
qqline(data2$X)
qqnorm(data2$Y, main = "Y Before Scale")
qqline(data2$Y)
qqnorm(data2$Z, main = "Z Before Scale")
qqline(data2$Z)
qqnorm(new_data$mutation, main = "Mut After Scale")
qqline(new_data$mutation)
qqnorm(new_data$x, main = "X After Scale")
qqline(new_data$x)
qqnorm(new_data$y, main = "Y After Scale")
qqline(new_data$y)
qqnorm(new_data$z, main = "Z After Scale")
qqline(new_data$z)

```

##K-means cluster

Clustering - Model validation (Find best parameter)

```{r,echo=FALSE}

#Model validation - find best parameter
index <- data.frame()

#topnum = 20
#file_names <- c("output_1.csv", "output_2.csv", "output_3.csv", "output_4.csv",, "output_5.csv", "output_6.csv", "output_7.csv", "output_8.csv", "output_9.csv", "output_10.csv", "output_11.csv", "output_12.csv", "output_13.csv", "output_14.csv", "output_15.csv", "output_16.csv", "output_17.csv", "output_18.csv", "output_19.csv", "output_20.csv")
#file_names <- c("output_1.csv", "output_2.csv", "output_3.csv")


#K-num iteration
#Kmin <- 5
#Kmax <- 200
#iteration <- 20 #iteration number per k num 

for (i in Kmin:Kmax) {
  
  #nstart = 20 means perform k = i for 20 times and select the best cluster outcome among them
  kmeans_result <- kmeans(new_data, centers = i, nstart = iteration)
  
  wss <- sum(kmeans_result$withinss)
  
  km_stats <- cluster.stats(dist(data2),  kmeans_result$cluster)

  # Dunn index
  dunn <- km_stats$dunn
  # Calinski-Harabasz index
  ch <- km_stats$ch
  # Average silhouette width
  sil <- km_stats$avg.silwidth
  #Entropy index
  ent <- km_stats$entropy

  
  
  # number of noise points :Noise points are data points that do not belong to any specific cluster or are considered outliers
  km_stats$noisen
  # vector of clusterwise within cluster average distances. (这个是scaled version的average distance， non-scaled version到时候还得再算一下)
  km_stats$average.distance
  # Calculate the separation matrix. provides a summary of the between-cluster distances and can help identify clusters that are well-separated or overlapping.
  #The separation matrix is a symmetric matrix that quantifies the degree of separation between clusters based on some distance measure. 
  separation_matrix <- km_stats$separation.matrix
  
  index <- rbind(index, c(dunn, ch, sil, ent,wss))
  
  }

```

```{r,echo=FALSE}
cat("Displays the highest index number：", topnum, "\n")
cat("K Selection of the iteration attempt range：", Kmin, "--", Kmax, "\n")
cat("Number of clustering times per K cycle：", iteration)
```

#Line chart for Dunn Index

```{r,echo=FALSE}


colnames(index) <- c("Dunn", "Calinski-Harabasz", "Average_Silhouette_Width", "Entropy","Wss")

#par(mfrow = c(2, 2))

# Line chart for Dunn Index
plot(index$Dunn, type = "l", xlab = "Observation", ylab = "Dunn Index", main = "Dunn Index",xlim = c(0, length(index$Dunn) +1), ylim = c(min(index$Dunn)*0.95, max(index$Dunn)*1.2 ))

top_dun <- data.frame()

# Mark the maximum value and add it to the table
sorted <- sort(index$Dunn, decreasing = TRUE)
top_three <- head(sorted, topnum)

print(top_three)

for (i in 1:length(top_three)) {
  
  num <- which(index$Dunn == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
  
  top_dun <- rbind(top_dun, c(i,top_three[i]) )
}
colnames(top_dun) <- c("K-Num", "Dunn")
datatable(top_dun)

```

#Line chart for Calinski-Harabasz Index

```{r,echo=FALSE}

# Line chart for Calinski-Harabasz Index
plot(index$`Calinski-Harabasz`, type = "l", xlab = "Observation", ylab = "Calinski-Harabasz Index", main = "Calinski-Harabasz Index",xlim = c(0, length(index$`Calinski-Harabasz`) +1), ylim = c(min(index$`Calinski-Harabasz`)*0.95, max(index$`Calinski-Harabasz`)*1.2 ))

top_ch <- data.frame()

# Mark the maximum value and add it to the table
sorted <- sort(index$`Calinski-Harabasz`, decreasing = TRUE)
top_three <- head(sorted, topnum)

for (i in 1:length(top_three)) {
  num <- which(index$`Calinski-Harabasz` == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
  top_ch <- rbind(top_ch, c(num,top_three[i]) )
}

colnames(top_ch) <- c("K-Num", "CH")
datatable(top_ch)

```

#Line chart for average Silhouette Width

```{r,echo=FALSE}

# Line chart for Average Silhouette Width
plot(index$`Average_Silhouette_Width`, type = "l", xlab = "Observation", ylab = "Average Silhouette Width", main = "Average Silhouette Width",xlim = c(0, length(index$`Average_Silhouette_Width`) +1), ylim = c(min(index$`Average_Silhouette_Width`)*0.95, max(index$`Average_Silhouette_Width`)*1.2))

top_ASW <- data.frame()

# Mark the maximum value and add it to the table
sorted <- sort(index$`Average_Silhouette_Width`, decreasing = TRUE)
top_three <- head(sorted, topnum)

for (i in 1:length(top_three)) {
  num <- which(index$`Average_Silhouette_Width` == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
  top_ASW <- rbind(top_ASW, c(num,top_three[i]) )
}

colnames(top_ASW) <- c("K-Num", "CH")
datatable(top_ASW)

```

#Line chart for Entropy

```{r,echo=FALSE}

# Line chart for Entropy
plot(index$Entropy, type = "l", xlab = "Observation", ylab = "Entropy", main = "Entropy",xlim = c(0, length(index$Entropy) +1), ylim = c(min(index$Entropy)*0.95, max(index$Entropy)*1.2))


# Mark the maximum value and add it to the table
sorted <- sort(index$Entropy, decreasing = FALSE)
top_three <- head(sorted, topnum)

for (i in 1:length(top_three)) {
  num <- which(index$Entropy == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
}
```

#Line chart for Wss

```{r,echo=FALSE}

# Line chart for Wss
plot(index$Wss, type = "l", xlab = "Observation", ylab = "Wss", main = "Elbow method",xlim = c(0, length(index$Wss) +1), ylim = c(min(index$Wss)*0.95, max(index$Wss)*1.2))


# Mark the maximum value and add it to the table
sorted <- sort(index$Wss, decreasing = FALSE)
top_three <- head(sorted,topnum)

for (i in 1:length(top_three)) {
  num <- which(index$Wss == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  for (n in num){
    points(x = n, y= top_three[i], col = "red", pch = 19)
  }
}

#data2$cluster <- as.factor(kmeans_result$cluster)


```

Dunn:the higher the better (well-separated and compact)

CH:the higher the better (well-separated and compact)

S:The higher the better (well-separated and compact)

Entropy: the lower the better

Wss: the lower the better
#weighted mean score table

weighted mean score = Dunn percentage + CH percentage + S percentage + entropy percentage + Wss percentage

P.S.:This is a quite naive score, I should keep learning to find a better way

```{r,echo=FALSE}

D <- index$Dunn
A <- index$`Average_Silhouette_Width`
CH <- index$`Calinski-Harabasz`
E <- index$Entropy
Wss <- index$Wss


WS <- data.frame()

for (i in 1:length(D)){
  
  weight <- D[i]/max(D) + A[i]/max(A) + CH[i]/max(CH) + (1-E[i]/max(E)) + (1-Wss[i]/max(Wss))
  
  WS <- rbind(WS, c(i+Kmin -1 ,weight) )
  
}

colnames(WS) <- c("K-Num", "Weighted score")
datatable(WS)

# Line chart for WS
plot(x=WS$`K-Num` ,y=WS$`Weighted score`, type = "l", xlab = "K-num", ylab = "Weighted score", main = "Weighted score",xlim = c(Kmin, Kmin+length(WS$`Weighted score`)-1), ylim = c(min(WS$`Weighted score`)*0.95, max(WS$`Weighted score`)*1.2))

Best_K <- data.frame()

# Mark the maximum value and add it to the table
sorted <- sort(WS$`Weighted score`, decreasing = TRUE)
top_three <- head(sorted,topnum)

for (i in 1:length(WS$`Weighted score`)) {
  num <- which(WS$`Weighted score` == top_three[i])
  #text(x = num, y = top_three[i]*1.05, labels = paste0("T", i, ": ", round(top_three[i],5)), offset = 1)
  for (n in num){
    points(x = n +Kmin -1, y= top_three[i], col = "red", pch = 19)
    Best_K <- rbind(Best_K, c(i,n+WS$`K-Num`[1] - 1,top_three[i]) )
    
    
  }
}

colnames(Best_K) <- c("Best_descend", "K-num","Weighted score")
datatable(Best_K)

```

Therefore,we get Best-K from the cluster indexes.
##Model validation
Now we need to perform validation whether these k numbers are significant compared to other k.

#Method 1: rank percent hypothesis test + min-max normalization

```{r,warning=FALSE}
alpha <- 0.05
k_validated <- data.frame()

for (k in Best_K[,2]){
  # Permutation method (rank percentage hypothesis test + feature scaling)
  # Compute the clustering results of the raw data
  orig_clusters <- kmeans(new_data, k)
  data3 <- data[1:972, c("X", "Y", "Z", "virusPercent")]
  orig_means <- tapply(new_data$mutation, orig_clusters$cluster, mean)
  
  # Calculate the percentile rank of the average mutation frequency for each cluster in the original data
  orig_pcts <- round(rank(orig_means) / length(orig_means) * 100, 2)
  
  # Perform 2000 mutation probability rearrangements and calculate the results for each cluster
  n_permutations <- 2000
  perm_results <- matrix(NA, n_permutations, length(orig_means))
  for (i in 1:n_permutations) {
    perm_data <- data3
    perm_data[, ncol(data3)] <- sample(data3[, ncol(data3)], replace = FALSE)
    mut_scale <- (perm_data[, ncol(data3)] - min(perm_data[, ncol(data3)])) / (max(perm_data[, ncol(data3)]) - min(perm_data[, ncol(data3)]))
    sta_scale <- data.frame(x = data2$x, y = data2$y, z = data2$z, mutation = mut_scale)
    perm_clusters <- kmeans(sta_scale, k)
    
    colnames(sta_scale) <- c("x", "y", "z", "mutation")
    perm_means <- tapply(sta_scale$mutation, perm_clusters$cluster, mean)
    perm_pcts <- round(rank(perm_means) / length(perm_means) * 100, 2)
    perm_results[i, ] <- perm_pcts
  }
  
  # Calculate p-values for each cluster
  p_values <- numeric(length(orig_means))
  for (i in 1:length(orig_means)) {
    p_values[i] <- mean(perm_results[, i] >= orig_pcts[i])
  }
  
  # Filter clusters based on p-values
  if (p_values[which.max(orig_means)] < alpha) {
    cat("Reject H0: The p-value is significant.")
    k_validated <- rbind(k_validated, k)
  } else {
    cat("Do not reject H0: The p-value is not significant.")
  }
}
```
Code above is used to calculate the percentile of each cluster in the rearrangement result.

Specifically, the rank() function calculates the rank of each cluster mean in the rearrangement result,

Then divide by the total number of rearrangements and multiply by 100,

The percentile of each cluster in the rearrangement result is obtained.

Finally, the round() function keeps the result two decimal places and rounds it.

This percentile can be used to assess whether each cluster in the original clustering result is significantly more likely than the random clustering result, will be used in step 6 to calculate the p-value.

Save the percentiles of the clustering results from each rearrangement in the perm_results matrix.

Specifically, each row of the perm_results matrix corresponds to a rearrangement of the result, each column corresponds to the percentile of a cluster.

In the for loop, the percentiles of the three clusters obtained by the i th rearrangement are stored in the perm_pcts vector.

Then assign the perm_pcts vector to row i of the perm_results matrix,

Thus storing the clustering results of this rearrangement in the perm_results matrix.

Calculate the p-value of each cluster

Check whether the mean of each class in the original data is significantly higher than the mean of the original data.

Specifically, the null hypothesis of this hypothesis test is that the mean of each class in the original data is equal to the mean of the original data, i.e. there is no significant difference.

Alternative hypothesis: The mean of each class in the original data is significantly higher than the mean in the original data

```{r}
# Calculate p-values for each cluster
p_values <- sapply(orig_means, function(mean_value) mean(perm_results[, ] >= mean_value))

# Validate clusters based on p-values
if (p_values[which.max(orig_means)] < alpha) {
  cat("The p-value is significant, rejecting the null hypothesis.")
  k_validated <- rbind(k_validated, k)
} else {
  cat("The p-value is not significant, not rejecting the null hypothesis.", "\n")
}

# Check if the cluster with the highest average mutation frequency is significant
if (p_values[which.max(orig_means)] < 0.05) {
  # Print the points in the significant cluster
  selected_rows <- which(orig_clusters$cluster == which.max(orig_means))
  for (i in selected_rows) {
    # Print the details of each point
    # cat(paste(i, "\t", new_data[i, "x"], "\t", new_data[i, "y"], "\t", new_data[i, "z"], "\t", data[i, "virusPercent"], "\n"))
  }
  
  # Visualize all points (in blue) and highlighted points (in red)
  significant_points <- which(orig_clusters$cluster == which.max(orig_means))
  significant_coordinates <- new_data[significant_points, c("x", "y", "z")]
  
  # Visualize the data points
  # ...
}

# Add a column to 'means' indicating high mutation points
col_name <- paste("Is_high")
means[, col_name] <- ifelse(means[, 2] > quantile(Per_means, probs = alp, na.rm = TRUE), 1, 0)

```
The code calculates p-values by comparing the original cluster means with the permutation results.

It validates the clusters based on the p-value of the cluster with the highest average mutation frequency.

If the p-value is significant (less than the significance level), it rejects the null hypothesis.

It checks if the cluster with the highest average mutation frequency is significant (p-value less than 0.05) and prints the details of the points in that cluster.

It visualizes the data points, highlighting the significant cluster if applicable.

Finally, it adds a column to 'means' indicating high mutation points based on the 95th percentile of the permutation results.

#Method 2 : rank percent hypothesis test + z-norm normalization

```{r,warning=FALSE}

#alpha = 0.05

k_validated <- data.frame()

for (k in Best_K[,2]){
  
#Permutation method (rank percentage hypothesis test + normalization)
# Compute the clustering junctions of the raw data
orig_clusters <- kmeans(scale_vector, k)
data3<-data[1:972,c("X","Y","Z","virusPercent")]
orig_means <- tapply(scale_vector$V4, orig_clusters$cluster, mean)

# This step is to calculate the percentile of the average mutation frequency for each class in the original data.

orig_pcts <- round(rank(orig_means)/length(orig_means)*100, 2)

n_permutations <- 50
perm_results <- matrix(NA, n_permutations, length(orig_means))
x_scale <- scale(data3[,1])
y_scale <- scale(data3[,2])
z_scale <- scale(data3[,3])
for (i in 1:n_permutations) {
  perm_data <- data3
  perm_data[,ncol(data3)] <- sample(data3[,ncol(data3)], replace=FALSE)
  mut_scale <- scale(perm_data[,ncol(data3)])
  sta_scale <- data.frame(x = x_scale,y = y_scale,z = z_scale,mutation = mut_scale)
  perm_clusters <- kmeans(sta_scale, k)
  
  colnames(sta_scale) <- c("x", "y","z","mutation")
  
  perm_means <- tapply(sta_scale$mutation, perm_clusters$cluster, mean)
  perm_pcts <- round(rank(perm_means)/length(perm_means)*100, 2)
  # Calculate percentile ranks of each cluster in the reordering results
perm_results[i,] <- perm_pcts
# Save the percentile ranks of each cluster from each permutation in the perm_results matrix

# Calculate p-values for each cluster
p_values <- numeric(length(orig_means))
for (i in 1:length(orig_means)) {
  p_values[i] <- mean(perm_results[,i] >= orig_pcts[i])
}
# Calculate the proportion of permutation results with percentile ranks higher than the original cluster percentile ranks

# Filter clusters based on p-values
if (p_values[which.max(orig_means)] < alpha) {
  cat("Reject H0: The p-value is significant.")
  k_validated <- rbind(k_validated, k)
} else {
  cat("Do not reject H0: The p-value is not significant.")
}

# Explanation of the code:
# - The code calculates the percentile ranks of each cluster in the reordering results.
# - It then computes p-values to test if the average mutation frequency of each cluster is significantly higher than the overall average.
# - If the p-value for the cluster with the highest average mutation frequency is below the significance level (alpha), it is considered significant.
# - The clusters with significant p-values are stored in the k_validated variable.

# Check if the p-value for the cluster with the highest average mutation frequency is significant
if (p_values[which.max(orig_means)] < 0.05) {
  # Select rows corresponding to the cluster with the highest average mutation frequency
  selected_rows <- which(orig_clusters$cluster == which.max(orig_means))
  # Iterate over the selected rows and print the corresponding data points
  for (i in selected_rows) {
    # Print the data points (row number, X, Y, Z, virusPercent)
    # cat(paste(i, "\t", scale_vector[i, "X"], "\t", scale_vector[i, "Y"], "\t", scale_vector[i, "Z"], "\t", scale_vector[i, "V4"], "\n"))
  }
  
  # Visualize all points (blue) and the significant points (red)
  significant_points <- which(orig_clusters$cluster == which.max(orig_means))
  significant_coordinates <- scale_vector[significant_points, c("X", "Y", "Z")]
  
  fig <- plot_ly() %>%
    add_trace(data = scale_vector, x = ~X, y = ~Y, z = ~Z, type = "scatter3d", mode = "markers", name = "All Points") %>%
    add_trace(data = significant_coordinates, x = ~X, y = ~Y, z = ~Z, type = "scatter3d", mode = "markers", name = "Significant Clusters", marker = list(color = "red")) %>%
    layout(scene = list(xaxis = list(title = "X"), yaxis = list(title = "Y"), zaxis = list(title = "Z")))
  
  fig
}

# Extract data points from the cluster with the highest average mutation frequency
cluster_data <- scale_vector[orig_clusters$cluster == which.max(orig_means), ]
# print(cluster_data)

per_num <- 50 
#alp <- 0.95
temp_data <- new_data

#select k onlt when p is small enough
if (length(k_validated) == 0) {
  Final_K <- 0
} else {
colnames(k_validated) <- c("K")

# select the repeated rows from K-num and K_validated  
Final_K <- Best_K[Best_K$`K-num` %in% k_validated$`K`, ]

datatable(Final_K)


    
for (k in Final_K[,2]){
  
data2<-data[1:972,c("atomSerialNumber","resID","X","Y","Z","virusPercent")]
data2 <- data.frame(data2)

kmeans_res <- kmeans(temp_data, centers=k, nstart=iteration)

result <- cbind(temp_data,kmeans_res$cluster)

colnames(result) <- c("X","Y","Z", "VP", "Cluster_Index")

result <- data.frame(result)

means <- data.frame()

for (n in 1:k){
  
  # Select rows in 'result' data frame where 'Cluster_Index' equals 72
  subset_result <- result[result$Cluster_Index == n, ]
  
  means <- rbind(means, c(n,sum(subset_result$VP)/nrow(subset_result))) 
  
}

colnames(means) <- c("Cluster_Index","mean_VP")


Per_means <- data.frame(means[,1])

colnames(Per_means) <- c("Cluster_Index")

result_Per <- data.frame(result)

for (i in 1:per_num){
  
  result_Per$VP <- sample(result_Per$VP)
  
  # Create a new column in 'Per_means' for the current iteration
  col_name <- paste("mean_", i, sep = "")
  Per_means[, col_name] <- NA
  
  for (n in 1:k){
  
  # Select rows in 'result' data frame where 'Cluster_Index' equals 72
  subset_result <- result_Per[result_Per$Cluster_Index == n, ]
  
  
  # Compute mean and add to 'Per_means' data frame with column name
  
  Per_means[n, col_name] <- sum(subset_result$VP)/nrow(subset_result)
  
}
  
}




# Create a new column in 'means' for identifying the high_mutation point
  col_name <- paste("Is_high")
  means[, col_name] <- NA


for (i in 1:k){
  
  # Compute the 95th percentile of the first row of 'Per_means'
  top_5_percentile <- quantile(Per_means[i, ], probs = alp, na.rm = TRUE)

  if (means[i,2] > top_5_percentile){
    
    means[i,3] <- 1
    
  }else{
    
    means[i,3] <- 0
    
  }
  
  
}



# Select rows in 'means' where 'Is_high' is equal to 1
high_mut <- means[means$Is_high == 1, ]

# Print the selected rows
#print(high_mut)

# Copy 'Cluster_Index' column from 'result' to 'data2'
data2 <- cbind(data2, result$Cluster_Index)

colnames(data2) <- c("atomSerialNumber","resID","X","Y","Z","virusPercent","Cluster_Index")

# Create a new column in 'data2' for identifying the high_mutation point
col_name <- paste("Is_high")
data2[, col_name] <- NA

# Create a new column 'Is_high' in 'data2'
data2$Is_high <- ifelse(data2$Cluster_Index %in% high_mut$Cluster_Index, 1, 0)

high_virus_data <- 0

# Select rows of 'data2' where 'Is_high' is equal to 1
high_virus_data <- data2[data2$Is_high == 1, ]

# Print the selected rows
#print(high_virus_data)

# Sort 'high_virus_data' by 'Cluster_Index'
high_virus_data <- high_virus_data[order(high_virus_data$Cluster_Index), ]

# Print the sorted 'high_virus_data' data frame
print(high_virus_data)
write.csv(high_virus_data, file = file_names[which(Final_K[, 2] == k)], row.names = FALSE)

#High_mut_center: a datarame having the cluster_index and the center coordinate
High_mut_center <- data.frame(matrix(nrow=nrow(high_mut), ncol=4))

# Create a new column in 'High_mut_center' for identifying the high_mutation point
colnames(High_mut_center) <- c("Cluster_Index","center_X","center_Y","center_Z")

High_mut_center[,1] <- high_mut[,1]

num <- 0
for (i in unique(high_virus_data$Cluster_Index)){
  num <- num+1
  temp <- high_virus_data[high_virus_data$Cluster_Index == i, ]
  
  x <- temp$X
  y <- temp$Y
  z <- temp$Z
  
  # Calculate the center of the three sets of x, y, and z coordinates
  center_X <- mean(x)
  center_Y <- mean(y)
  center_Z <- mean(z)
  
  #print(center_X)
  
  High_mut_center[num,2] <- center_X
  High_mut_center[num,3] <- center_Y
  High_mut_center[num,4] <- center_Z
  
}

# Print the center coordinates
cat("K-num:", k,"\n")
cat("Hot spot centers are:","\n")
print(High_mut_center)
}
}
```